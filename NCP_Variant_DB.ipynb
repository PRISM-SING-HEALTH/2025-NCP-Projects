{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRISM-SING-HEALTH/2025-NCP-Projects/blob/main/NCP_Variant_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "FaS8PvOOncrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains Python standard imports required for the program.\n",
        "\n"
      ],
      "metadata": {
        "id": "keKog42wnkb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrOlgb8fZ_2M",
        "outputId": "c0ae7c92-480b-48d8-ff34-e159690e8330"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import requests"
      ],
      "metadata": {
        "id": "Kn4RxO80npey"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains Python imports used for unit testing and are not required for the main program."
      ],
      "metadata": {
        "id": "H6Z3r-NrnraM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "Z-woNJnevcob"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "from unittest.mock import patch, MagicMock\n",
        "import tempfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4UL2tWf3fgb"
      },
      "source": [
        "# functions.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFmHoNpn36Oj"
      },
      "source": [
        "This section contains Python functions along with their documentation, that are used in the main program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "BoDuKlkc34Wk"
      },
      "outputs": [],
      "source": [
        "def load_config(config_file):\n",
        "    \"\"\"\n",
        "    Loads configuration from a YAML file and validates required keys.\n",
        "\n",
        "    Args:\n",
        "        config_file (str): Path to the YAML configuration file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Configuration settings.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the YAML configuration file is not found.\n",
        "        ValueError: If the YAML file cannot be parsed or required keys are missing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(config_file, 'r') as file:\n",
        "            config = yaml.safe_load(file)\n",
        "\n",
        "        # Validate required keys\n",
        "        required_keys = ['base_path', 'local_path', 'files']\n",
        "        missing_keys = [key for key in required_keys if key not in config]\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required configuration keys: {', '.join(missing_keys)}\")\n",
        "\n",
        "        print(\"Configuration loaded successfully.\")\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The configuration file '{config_file}' was not found.\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error: Failed to parse the YAML configuration file. Details: {e}\")\n",
        "        raise\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "9KvTuU8M8WP9"
      },
      "outputs": [],
      "source": [
        "def standardise_dataframe(df, columns):\n",
        "  \"\"\"\n",
        "  Converts unstandardised dataframe to standardised dataframe.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "    columns (list): List of standardised column names.\n",
        "\n",
        "  Returns:\n",
        "    standardised_df (pd.DataFrame): Standardized dataframe.\n",
        "  \"\"\"\n",
        "  standardised_df = df.reindex(columns = columns, fill_value = np.nan)\n",
        "  return standardised_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query(df):\n",
        "  \"\"\"\n",
        "  Queries dataframe based on user input. Has case insensitive comparison.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "\n",
        "  Returns:\n",
        "    query_result (pd.DataFrame): Query result.\n",
        "  \"\"\"\n",
        "  print('\\n### Query Dataframe ###')\n",
        "  print(f\"Available categories: {', '.join(df.columns)}\\n\")\n",
        "\n",
        "  try:\n",
        "    # User input category\n",
        "    category = input('Enter category: ').strip()\n",
        "    category_lower = category.lower() # convert to lower case for non-case sens.\n",
        "\n",
        "    # Check if category exists\n",
        "    categories_lower = [col.lower() for col in df.columns]\n",
        "    if category_lower not in categories_lower:\n",
        "      raise ValueError(f\"Invalid category '{category}'. Please choose from available categories.\")\n",
        "\n",
        "    # Find actual column name (case sens match)\n",
        "    category_actual = df.columns[categories_lower.index(category_lower)]\n",
        "\n",
        "    # User input item (case insensitive and substring search)\n",
        "    item = input(f\"Enter item in category '{category_actual}': \").strip()\n",
        "\n",
        "    # Handle empty item input\n",
        "    if not item:\n",
        "      print(\"Error: item cannot be empty.\")\n",
        "      return None\n",
        "\n",
        "    # Query dataframe with case insensitive comparison\n",
        "    query_result = df[df[category_actual].astype(str).str.contains(item, case = False, na = False)]\n",
        "\n",
        "    if query_result.empty:\n",
        "      print(f\"No results found for item containing '{item}' in category '{category_actual}'.\")\n",
        "    else:\n",
        "      print(f\"Query Results for item ({len(query_result)} records(s) found):\")\n",
        "      return query_result\n",
        "\n",
        "  except ValueError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "BDKho2W2cJ43"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_excel(df, output_path):\n",
        "    \"\"\"\n",
        "    Exports the given DataFrame to an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to export.\n",
        "        output_path (str): Path where the Excel file will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_filled = df.fillna('NA')\n",
        "        df_filled.to_excel(output_path, index=False)\n",
        "        print(f\"Data successfully exported to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while exporting to Excel: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "cpELaUKfkG_d"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_variants(file_path, output_path):\n",
        "    \"\"\"\n",
        "    Validates genetic variants using the VariantValidator API (hgvs2reference endpoint).\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input Excel file containing variants.\n",
        "        output_path (str): Path to save the validation results.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the exported data\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        # Ensure the required column is present\n",
        "        if 'HGVSc' not in df.columns:\n",
        "            raise ValueError(\"Missing required column: 'HGVSc'\")\n",
        "\n",
        "        # Prepare to store validation results\n",
        "        validation_results = []\n",
        "\n",
        "        # Iterate over rows to validate each variant\n",
        "        print(\"Validating variants...\")\n",
        "        for index, row in df.iterrows():\n",
        "            try:\n",
        "                hgvs_c = row['HGVSc']\n",
        "\n",
        "                # Construct the API URL\n",
        "                api_url = f\"https://rest.variantvalidator.org/VariantValidator/tools/hgvs2reference/{hgvs_c}?content-type=text/xml\"\n",
        "\n",
        "                # Make the API request\n",
        "                response = requests.get(api_url, headers={\"accept\": \"text/xml\"})\n",
        "                if response.status_code == 200:\n",
        "                    # Successful validation\n",
        "                    validation_results.append({\n",
        "                        'HGVSc': hgvs_c,\n",
        "                        'Validation Status': 'Success',\n",
        "                        'Message': 'Validation successful'\n",
        "                    })\n",
        "                elif response.status_code == 404:\n",
        "                    # Variant not found\n",
        "                    validation_results.append({\n",
        "                        'HGVSc': hgvs_c,\n",
        "                        'Validation Status': 'Failed',\n",
        "                        'Message': 'Variant not found (404). Check HGVS notation.'\n",
        "                    })\n",
        "                else:\n",
        "                    # Other errors\n",
        "                    validation_results.append({\n",
        "                        'HGVSc': hgvs_c,\n",
        "                        'Validation Status': 'Failed',\n",
        "                        'Message': f\"Error {response.status_code}: {response.text}\"\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                validation_results.append({\n",
        "                    'HGVSc': row['HGVSc'],\n",
        "                    'Validation Status': 'Error',\n",
        "                    'Message': f\"Error during validation: {e}\"\n",
        "                })\n",
        "\n",
        "        # Save results to an output file\n",
        "        results_df = pd.DataFrame(validation_results)\n",
        "        results_df.to_excel(output_path, index=False)\n",
        "        print(f\"Validation results saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "LdiewUjfxykj"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVt3VlqC3Spq"
      },
      "source": [
        "# test_cases.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA0zVkZe4MNC"
      },
      "source": [
        "This section contains test cases for each function, using Python in-built 'unittest' framework. The test cases ensures the function works as intended for majority of user cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "7OnayuWx41Gc"
      },
      "outputs": [],
      "source": [
        "class TestLoadConfig(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Set up temporary files and test data.\n",
        "        \"\"\"\n",
        "        self.valid_config = {\n",
        "            'base_path': '/data/',\n",
        "            'local_path': '/local/',\n",
        "            'files': {\n",
        "                'file1': 'file1.xlsx',\n",
        "                'file2': 'file2.xlsx'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.missing_keys_config = {\n",
        "            'base_path': '/data/',\n",
        "            'files': {\n",
        "                'file1': 'file1.xlsx'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create temporary YAML files for testing\n",
        "        self.valid_config_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.valid_config_file.write(yaml.dump(self.valid_config).encode())\n",
        "        self.valid_config_file.close()\n",
        "\n",
        "        self.missing_keys_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.missing_keys_file.write(yaml.dump(self.missing_keys_config).encode())\n",
        "        self.missing_keys_file.close()\n",
        "\n",
        "        self.invalid_yaml_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.invalid_yaml_file.write(b\"{invalid_yaml: [missing, closing, brace\")\n",
        "        self.invalid_yaml_file.close()\n",
        "\n",
        "        self.nonexistent_file = '/nonexistent/config.yaml'\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"\n",
        "        Clean up temporary files after tests.\n",
        "        \"\"\"\n",
        "        os.remove(self.valid_config_file.name)\n",
        "        os.remove(self.missing_keys_file.name)\n",
        "        os.remove(self.invalid_yaml_file.name)\n",
        "\n",
        "    def test_valid_config(self):\n",
        "        \"\"\"\n",
        "        Test that a valid configuration file is loaded successfully.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid config file loaded.\")\n",
        "        config = load_config(self.valid_config_file.name)\n",
        "        self.assertEqual(config, self.valid_config)\n",
        "\n",
        "    def test_missing_keys(self):\n",
        "        \"\"\"\n",
        "        Test that a configuration file with missing keys raises a ValueError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Missing keys.\")\n",
        "        with self.assertRaises(ValueError) as context:\n",
        "            load_config(self.missing_keys_file.name)\n",
        "        self.assertIn(\"Missing required configuration keys\", str(context.exception))\n",
        "\n",
        "    def test_invalid_yaml(self):\n",
        "        \"\"\"\n",
        "        Test that an invalid YAML file raises a yaml.YAMLError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Invalid YAML file.\")\n",
        "        with self.assertRaises(yaml.YAMLError):\n",
        "            load_config(self.invalid_yaml_file.name)\n",
        "\n",
        "    def test_file_not_found(self):\n",
        "        \"\"\"\n",
        "        Test that a non-existent file raises a FileNotFoundError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: File not found.\")\n",
        "        with self.assertRaises(FileNotFoundError):\n",
        "            load_config(self.nonexistent_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "q0r_1igA8c3l"
      },
      "outputs": [],
      "source": [
        "class TestStandardiseDataFrame(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Define a sample DataFrame and standard columns\n",
        "        self.input_df = pd.DataFrame({\n",
        "            \"A\": [1, 2, 3],\n",
        "            \"B\": [4, 5, 6],\n",
        "            \"C\": [7, 8, 9]\n",
        "        })\n",
        "\n",
        "        self.standard_columns = [\"A\", \"B\", \"C\", \"D\", \"E\"]  # Standardised columns to test against\n",
        "\n",
        "    def test_standardise_dataframe_adds_missing_columns(self):\n",
        "        \"\"\"\n",
        "        Test that missing columns are added with NaN values.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Missing columns added with NaN values.\")\n",
        "        result_df = standardise_dataframe(self.input_df, self.standard_columns)\n",
        "\n",
        "        # Check that the columns in the result match the standard columns\n",
        "        self.assertListEqual(list(result_df.columns), self.standard_columns)\n",
        "\n",
        "        # Check that the new columns contain NaN\n",
        "        self.assertTrue(result_df[\"D\"].isnull().all())\n",
        "        self.assertTrue(result_df[\"E\"].isnull().all())\n",
        "\n",
        "    def test_standardise_dataframe_removes_extra_columns(self):\n",
        "        \"\"\"\n",
        "        Test that extra columns in the input DataFrame are removed.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Extra columns in input dataframe are removed.\")\n",
        "        result_df = standardise_dataframe(self.input_df, [\"A\", \"B\"])  # Subset of standard columns\n",
        "\n",
        "        # Check that only the specified columns remain\n",
        "        self.assertListEqual(list(result_df.columns), [\"A\", \"B\"])\n",
        "\n",
        "    def test_standardise_dataframe_keeps_original_data(self):\n",
        "        \"\"\"\n",
        "        Test that the original data in the columns is preserved.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Original data in columns is preserved.\")\n",
        "        result_df = standardise_dataframe(self.input_df, self.standard_columns)\n",
        "\n",
        "        # Check that data in original columns remains unchanged\n",
        "        pd.testing.assert_series_equal(result_df[\"A\"], self.input_df[\"A\"])\n",
        "        pd.testing.assert_series_equal(result_df[\"B\"], self.input_df[\"B\"])\n",
        "        pd.testing.assert_series_equal(result_df[\"C\"], self.input_df[\"C\"])\n",
        "\n",
        "    def test_standardise_dataframe_empty_dataframe(self):\n",
        "        \"\"\"\n",
        "        Test behavior with an empty input DataFrame.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Empty input dataframe.\")\n",
        "        empty_df = pd.DataFrame()\n",
        "        result_df = standardise_dataframe(empty_df, self.standard_columns)\n",
        "\n",
        "        # Check that all standard columns are present and filled with NaN\n",
        "        self.assertListEqual(list(result_df.columns), self.standard_columns)\n",
        "        self.assertTrue(result_df.isnull().all().all())\n",
        "\n",
        "    def test_standardise_dataframe_no_columns(self):\n",
        "        \"\"\"\n",
        "        Test behavior when no columns are specified.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: No column specified.\")\n",
        "        result_df = standardise_dataframe(self.input_df, [])\n",
        "\n",
        "        # Check that the resulting DataFrame has no columns\n",
        "        self.assertTrue(result_df.empty)\n",
        "        self.assertListEqual(list(result_df.columns), [])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestQueryFunction(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample DataFrame for testing\n",
        "        self.df = pd.DataFrame({\n",
        "            'CategoryA': ['Apple', 'Banana', 'Cherry', 'Apple Pie'],\n",
        "            'CategoryB': ['Dog', 'Cat', 'Horse', 'Fish']\n",
        "        })\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', 'Apple'])\n",
        "    def test_valid_query_case_insensitive(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test valid query with case-insensitive comparison.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid query with case-insensitive comparison\")\n",
        "        result = query(self.df)\n",
        "        expected = self.df[self.df['CategoryA'].str.contains('Apple', case=False, na=False)]\n",
        "        pd.testing.assert_frame_equal(result, expected)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryB', 'cat'])\n",
        "    def test_valid_query_substring_search(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test valid query with substring matching.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid query with substring matching\")\n",
        "        result = query(self.df)\n",
        "        expected = self.df[self.df['CategoryB'].str.contains('cat', case=False, na=False)]\n",
        "        pd.testing.assert_frame_equal(result, expected)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['InvalidCategory', 'Apple'])\n",
        "    def test_invalid_category(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying an invalid category.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Invalid category\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', 'NonexistentItem'])\n",
        "    def test_no_results_found(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying an item that doesn't exist in the DataFrame.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: No results found.\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', ''])\n",
        "    def test_empty_item(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying with an empty item.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Empty item.\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)"
      ],
      "metadata": {
        "id": "vIH0-DPgdfXw"
      },
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestExportToExcel(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample DataFrame for testing\n",
        "        self.sample_df = pd.DataFrame({\n",
        "            \"A\": [1, 2, None],\n",
        "            \"B\": [\"Test\", None, \"Data\"]\n",
        "        })\n",
        "        self.output_path = \"test_output.xlsx\"\n",
        "\n",
        "    @patch(\"pandas.DataFrame.to_excel\")\n",
        "    def test_export_to_excel_success(self, mock_to_excel):\n",
        "        \"\"\"\n",
        "        Test successful export of DataFrame to an Excel file.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Export dataframe to excel file.\")\n",
        "        # Call the function\n",
        "        export_to_excel(self.sample_df, self.output_path)\n",
        "\n",
        "        # Assert that `to_excel` was called with correct arguments\n",
        "        mock_to_excel.assert_called_once_with(self.output_path, index=False)\n",
        "\n",
        "    @patch(\"pandas.DataFrame.to_excel\", side_effect=PermissionError(\"Permission denied\"))\n",
        "    def test_export_to_excel_permission_error(self, mock_to_excel):\n",
        "        \"\"\"\n",
        "        Test export when a PermissionError occurs.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Export with PermissionError.\")\n",
        "        with self.assertRaises(PermissionError):\n",
        "            export_to_excel(self.sample_df, self.output_path)\n",
        "\n",
        "    def tearDown(self):\n",
        "        # Clean up the test output file if it was accidentally created\n",
        "        if os.path.exists(self.output_path):\n",
        "            os.remove(self.output_path)"
      ],
      "metadata": {
        "id": "aZzkF2kRkQ-P"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Run tests without exiting"
      ],
      "metadata": {
        "id": "hAb1njakfaCq",
        "outputId": "026581d4-d9fc-4678-acb5-bddc133c232c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "............"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Case: Export with PermissionError.\n",
            "An error occurred while exporting to Excel: Permission denied\n",
            "\n",
            "Test Case: Export dataframe to excel file.\n",
            "Data successfully exported to test_output.xlsx\n",
            "\n",
            "Test Case: File not found.\n",
            "Error: The configuration file '/nonexistent/config.yaml' was not found.\n",
            "\n",
            "Test Case: Invalid YAML file.\n",
            "Error: Failed to parse the YAML configuration file. Details: while parsing a flow sequence\n",
            "  in \"/tmp/tmpe43ybzb2.yaml\", line 1, column 16\n",
            "expected ',' or ']', but got '<stream end>'\n",
            "  in \"/tmp/tmpe43ybzb2.yaml\", line 1, column 40\n",
            "\n",
            "Test Case: Missing keys.\n",
            "Error: Missing required configuration keys: local_path\n",
            "\n",
            "Test Case: Valid config file loaded.\n",
            "Configuration loaded successfully.\n",
            "\n",
            "Test Case: Empty item.\n",
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: CategoryA, CategoryB\n",
            "\n",
            "Error: item cannot be empty.\n",
            "\n",
            "Test Case: Invalid category\n",
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: CategoryA, CategoryB\n",
            "\n",
            "Error: Invalid category 'InvalidCategory'. Please choose from available categories.\n",
            "\n",
            "Test Case: No results found.\n",
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: CategoryA, CategoryB\n",
            "\n",
            "No results found for item containing 'NonexistentItem' in category 'CategoryA'.\n",
            "\n",
            "Test Case: Valid query with case-insensitive comparison\n",
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: CategoryA, CategoryB\n",
            "\n",
            "Query Results for item (2 records(s) found):\n",
            "\n",
            "Test Case: Valid query with substring matching\n",
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: CategoryA, CategoryB\n",
            "\n",
            "Query Results for item (1 records(s) found):\n",
            "\n",
            "Test Case: Missing columns added with NaN values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.077s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Case: Empty input dataframe.\n",
            "\n",
            "Test Case: Original data in columns is preserved.\n",
            "\n",
            "Test Case: No column specified.\n",
            "\n",
            "Test Case: Extra columns in input dataframe are removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCeM1XPC3oPy"
      },
      "source": [
        "# main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0_nVt4o4kRG"
      },
      "source": [
        "This section contains the main program prototype, which will be later transferred over to VSCode once everything is working as intended. The first part loads the configuration from the YAML file and defines the standard columns for the program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "0JCnmuPNvniD",
        "outputId": "9d2e5620-799f-46cb-cad3-2ecd59a93d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from YAML file\n",
        "config = load_config('/content/drive/MyDrive/Variant_DB/Mock_Local_Drive/config.yaml')\n",
        "\n",
        "# Initialise file paths variables for excel files\n",
        "base_path = config['base_path']\n",
        "local_path = config['local_path']\n",
        "files = config['files']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "0_QRs2CDqtdq"
      },
      "outputs": [],
      "source": [
        "# Define standard columns\n",
        "standard_columns = [\n",
        "    'MRN',\n",
        "    'Patient Name',\n",
        "    'Phenotype',\n",
        "    'Solved Status',\n",
        "    'Gene',\n",
        "    'Transcript',\n",
        "    'Variant',\n",
        "    'HGVSg',\n",
        "    'HGVSc',\n",
        "    'HGVSp'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part imports the data into 5 separate, unsorted dataframes."
      ],
      "metadata": {
        "id": "iCMsCWSpo2NI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "QCnw8yNvEMj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f34e383-52b7-4573-993d-1a81980bd399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Performing additional filtering...\n",
            "Transforming into structured dataframe...\n",
            "Data import complete.\n"
          ]
        }
      ],
      "source": [
        "# Importing data into dataframes\n",
        "print('Importing data...')\n",
        "\n",
        "try:\n",
        "  # Lab Cases\n",
        "  lab_cases_df = pd.read_excel(\n",
        "      f\"{base_path}{files['lab_cases']}\",\n",
        "      sheet_name = 'Sheet1',\n",
        "      header = 0,\n",
        "      usecols = 'F, AF:AI, AL'\n",
        "  )\n",
        "\n",
        "  # ATM Summary\n",
        "  atm_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['atm_summary']}\",\n",
        "      sheet_name='SUMMARY',\n",
        "      header = None,\n",
        "      usecols = 'A:B'\n",
        "  )\n",
        "\n",
        "  # Additional filtering due to unconventional formatting\n",
        "  print('Performing additional filtering...')\n",
        "  atm_summary_df_filtered = atm_summary_df.iloc[9:16]  # Ensure rows are correctly indexed\n",
        "  atm_summary_df_filtered.reset_index(drop=True, inplace=True)  # Reset index for clean output\n",
        "\n",
        "  # Convert to a structured DataFrame\n",
        "  print('Transforming into structured dataframe...')\n",
        "  atm_structured_df = atm_summary_df_filtered.set_index(0).T  # Set column 0 as the header, transpose\n",
        "  atm_structured_df.reset_index(drop=True, inplace=True)  # Reset index for clean final DataFrame\n",
        "\n",
        "  # Invitae Summary\n",
        "  invitae_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['invitae_summary']}\",\n",
        "      sheet_name = 'Invitae list header',\n",
        "      header = 0,\n",
        "      usecols = 'E:F, M:T'\n",
        "  )\n",
        "\n",
        "  # Clinical Summary\n",
        "  clinical_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['clinical_summary']}\",\n",
        "      sheet_name = '11 Dec',\n",
        "      header = 0,\n",
        "      usecols = 'D:E, K')\n",
        "\n",
        "  # Research Summary\n",
        "  research_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['research_summary']}\",\n",
        "      sheet_name = 'Overall List',\n",
        "      header = 2,\n",
        "      usecols = 'B, W:X, AF, AS:AT, AV:AW')\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "  print(f\"File not found: {e}\")\n",
        "except ValueError as e:\n",
        "  print(f\"Error reading sheet or invalid data format: {e}\")\n",
        "\n",
        "print('Data import complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "pL1WIYZ4o-BZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "1TE9tZBvhLKI"
      },
      "outputs": [],
      "source": [
        "# Print dataframes\n",
        "#print('Printing dataframes...')\n",
        "\n",
        "#print('\\nLab Cases')\n",
        "#print(lab_cases_df)\n",
        "\n",
        "#print('\\nATM Summary')\n",
        "#print(atm_structured_df)\n",
        "\n",
        "#print('\\nInvitae Summary')\n",
        "#print(invitae_summary_df)\n",
        "\n",
        "#print('\\nClinical Summary')\n",
        "#print(clinical_summary_df)\n",
        "\n",
        "#print('\\nResearch Summary')\n",
        "#print(research_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses key-value pairs to rename the columns from the imported dataframes to the standardised columns."
      ],
      "metadata": {
        "id": "crVbsanWpGKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "DLRt0qwiIKBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9e5a9b-aac8-4f98-9992-c6bdaccc1714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardising columns across dataframes...\n"
          ]
        }
      ],
      "source": [
        "# Standardise columns across dataframes\n",
        "print('Standardising columns across dataframes...')\n",
        "\n",
        "# Lab Cases\n",
        "lab_cases_cols = {\n",
        "    'Number variants detected'    :'Var Count',\n",
        "    'Variant_1_gene'              :'Gene',\n",
        "    'Variant_1_HGVSg'             :'HGVSg',\n",
        "    'Variant_1_HGVSc'             :'HGVSc',\n",
        "    'Variant_1_HGVSp'             :'HGVSp',\n",
        "    'Variant_1_zygosity'          :'Zygosity',\n",
        "    'Variant_1_inheritance'       :'Inheritance',\n",
        "    'Variant_1_Validation_Status' :'Solved Status'\n",
        "}\n",
        "\n",
        "lab_cases_df.rename(columns = lab_cases_cols, inplace = True)\n",
        "\n",
        "# ATM Summary\n",
        "atm_structured_df_cols = {\n",
        "    'HGVS_Genomic_GRCh38/hg38'            :'HSVSg',\n",
        "    'HGVS_MANE Select_Transcript_RefSeq'  :'Transcript',\n",
        "    'HGVS_MANE Select_cDNA'               :'HGVSc',\n",
        "    'HGVS_MANE Select_protein'            :'HGVSp',\n",
        "    'HUGO gene symbol'                    :'Gene',\n",
        "}\n",
        "\n",
        "atm_structured_df.rename(columns = atm_structured_df_cols, inplace = True)\n",
        "\n",
        "# Invitae Summary\n",
        "invitae_summary_df_cols = {\n",
        "    'Patient ID (MRN)'  :'MRN',\n",
        "    'Patient Name'      :'Patient Name',\n",
        "    'Result'            :'Solved Status',\n",
        "    'Gene'              :'Gene',\n",
        "    'Transcript'        :'Transcript',\n",
        "    'Variant'           :'Variant',\n",
        "    'HGVSc'             :'HGVSc',\n",
        "    'Protein Change'    :'HGVSp',\n",
        "}\n",
        "\n",
        "invitae_summary_df.rename(columns = invitae_summary_df_cols, inplace = True)\n",
        "\n",
        "# Clinical Summary\n",
        "clinical_summary_df_cols = {\n",
        "    'Identification No.'        :'MRN',\n",
        "    'Medical Prob description'  :'Phenotype',\n",
        "    'Patient Name'              :'Patient Name'\n",
        "}\n",
        "\n",
        "clinical_summary_df.rename(columns = clinical_summary_df_cols, inplace = True)\n",
        "\n",
        "# Research Summary\n",
        "research_summary_df_cols = {\n",
        "    'IC No (MRN)'         :'MRN',\n",
        "    'Name'                :'Patient Name',\n",
        "    'Candidate gene (1)'  :'Gene',\n",
        "    'Transcript (1)'      :'Transcript',\n",
        "    'cDNA (1)'            :'HGVSc',\n",
        "    'Protein (1)'         :'HGVSp',\n",
        "    'AUTO STATUS'         :'Solved Status'\n",
        "}\n",
        "\n",
        "research_summary_df.rename(columns = research_summary_df_cols, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "iTKUSXvWpR1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "6x-Sz1WP8kvV"
      },
      "outputs": [],
      "source": [
        "# Print dataframes after renaming\n",
        "#print('Printing dataframes...')\n",
        "\n",
        "#print('\\nLab Cases')\n",
        "#print(lab_cases_df)\n",
        "\n",
        "#print('\\nATM Summary')\n",
        "#print(atm_structured_df)\n",
        "\n",
        "#print('\\nInvitae Summary')\n",
        "#print(invitae_summary_df)\n",
        "\n",
        "#print('\\nClinical Summary')\n",
        "#print(clinical_summary_df)\n",
        "\n",
        "#print('\\nResearch Summary')\n",
        "#print(research_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***standardise_dataframe*** function to sort all of the imported, renamed dataframes into the standardised format for easier integration into a singular dataframe later on."
      ],
      "metadata": {
        "id": "DaxEV5V-pp0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "qGfg0F1cxdkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330d4d17-7a61-440c-93f1-a31b8a67aa24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardising all dataframes...\n",
            "Standardisation complete.\n"
          ]
        }
      ],
      "source": [
        "print('Standardising all dataframes...')\n",
        "\n",
        "lab_cases_df_standard = standardise_dataframe(lab_cases_df, standard_columns)\n",
        "atm_structured_df_standard = standardise_dataframe(atm_structured_df, standard_columns)\n",
        "invitae_summary_df_standard = standardise_dataframe(invitae_summary_df, standard_columns)\n",
        "clinical_summary_df_standard = standardise_dataframe(clinical_summary_df, standard_columns)\n",
        "research_summary_df_standard = standardise_dataframe(research_summary_df, standard_columns)\n",
        "\n",
        "print('Standardisation complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "5CCkO49HqJVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "lrq6O3yrxlns"
      },
      "outputs": [],
      "source": [
        "# Print dataframes to verify\n",
        "#print('Individual dataframes:')\n",
        "#print('\\nLab Cases:')\n",
        "#print(lab_cases_df_standard.head())\n",
        "\n",
        "#print('\\nATM:')\n",
        "#print(atm_structured_df_standard.head())\n",
        "\n",
        "#print('\\nInvitae:')\n",
        "#print(invitae_summary_df_standard.head())\n",
        "\n",
        "#print('\\nClinical:')\n",
        "#print(clinical_summary_df_standard.head())\n",
        "\n",
        "#print('\\nResearch:')\n",
        "#print(research_summary_df_standard.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part combines all of the standardised dataframes into one singular dataframe."
      ],
      "metadata": {
        "id": "WVNB54deqL0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "xQd3sGenNIzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3958d219-bdae-4c1d-bb4d-d0fc24597b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combining dataframes...\n",
            "All dataframes combined successfully.\n",
            "Combined dataframe:\n",
            "        MRN Patient Name                                          Phenotype  \\\n",
            "0       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "1       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "2       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "3       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "4       NaN          NaN  Developmental / Behavioural - Delayed fine mot...   \n",
            "5       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "6       NaN          NaN  Developmental / Behavioural - Global developme...   \n",
            "7       NaN          NaN                                                NaN   \n",
            "8   43234.0    John Pork                                                NaN   \n",
            "9       NaN          NaN                                                NaN   \n",
            "10    420.0      Jon Doe  Shwachman-Diamond Syndrome\\nloose stools- fat ...   \n",
            "11      NaN          NaN  6 weeks pregnant\\n\\nHistory of:\\nVSD\\nAnal atr...   \n",
            "12   1234.0   John Smith                   GDD with regression, Autism, FTT   \n",
            "13      NaN          NaN  Dysmorphism, facial dysmorphism: arched eyebro...   \n",
            "\n",
            "        Solved Status    Gene   Transcript    Variant  \\\n",
            "0   Confirmed present   PRRT2          NaN        NaN   \n",
            "1   Confirmed present   PRRT2          NaN        NaN   \n",
            "2   Confirmed present  STING1          NaN        NaN   \n",
            "3   Confirmed present  DYRK1A          NaN        NaN   \n",
            "4   Confirmed present   NTRK1          NaN        NaN   \n",
            "5   Confirmed present  DYRK1A          NaN        NaN   \n",
            "6   Confirmed present  DYRK1A          NaN        NaN   \n",
            "7                 NaN     NaN  NM_000051.3        NaN   \n",
            "8            POSITIVE    TSC2  NM_000548.3  c.1513C>T   \n",
            "9            NEGATIVE       -            -          -   \n",
            "10                NaN     NaN          NaN        NaN   \n",
            "11                NaN     NaN          NaN        NaN   \n",
            "12             Solved   CDK13  NM_003718.5        NaN   \n",
            "13             Solved   KMT2D  NM_003482.3        NaN   \n",
            "\n",
            "                          HGVSg                    HGVSc  \\\n",
            "0           chr16:g.29813703dup     NM_145239.3:c.649dup   \n",
            "1           chr16:g.29813703dup     NM_145239.3:c.649dup   \n",
            "2   NC_000005.10:g.139480847C>T     NM_198282.4:c.463G>A   \n",
            "3           chr21:g.37480775G>A  NM_001347721.2:c.438G>A   \n",
            "4           chr1:g.156879282T>G    NM_002529.4:c.1966T>G   \n",
            "5           chr21:g.37480775G>A  NM_001347721.2:c.438G>A   \n",
            "6           chr21:g.37480775G>A  NM_001347721.2:c.438G>A   \n",
            "7                           NaN                c.3576G>A   \n",
            "8                           NaN                      NaN   \n",
            "9                           NaN                      NaN   \n",
            "10                          NaN                      NaN   \n",
            "11                          NaN                      NaN   \n",
            "12                          NaN                c.2389G>C   \n",
            "13                          NaN              c.12063dupG   \n",
            "\n",
            "                            HGVSp  \n",
            "0   NP_660282.2:p.Arg217ProfsTer8  \n",
            "1   NP_660282.2:p.Arg217ProfsTer8  \n",
            "2         NP_938023.1:p.Val155Met  \n",
            "3      NP_001334650.1:p.Trp146Ter  \n",
            "4         NP_002520.2:p.Cys656Gly  \n",
            "5      NP_001334650.1:p.Trp146Ter  \n",
            "6      NP_001334650.1:p.Trp146Ter  \n",
            "7                    p.(Lys1192=)  \n",
            "8                       p.Arg505*  \n",
            "9                               -  \n",
            "10                            NaN  \n",
            "11                            NaN  \n",
            "12                    p.Asp797His  \n",
            "13                      p.T4022fs  \n"
          ]
        }
      ],
      "source": [
        "# Combining dataframes into a single dataframe\n",
        "print('\\nCombining dataframes...')\n",
        "combined_df = pd.concat([\n",
        "    lab_cases_df_standard,\n",
        "    atm_structured_df_standard,\n",
        "    invitae_summary_df_standard,\n",
        "    clinical_summary_df_standard,\n",
        "    research_summary_df_standard\n",
        "], ignore_index = True)\n",
        "\n",
        "print('All dataframes combined successfully.')\n",
        "print('Combined dataframe:')\n",
        "print(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***query*** function for the user to initiate a query using the new singular dataframe."
      ],
      "metadata": {
        "id": "L8-AGsZDqU90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "u8dZt3bQKo7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb26ad1-e66d-4aa8-93dc-30e92d7e53d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Query Dataframe ###\n",
            "Available categories: MRN, Patient Name, Phenotype, Solved Status, Gene, Transcript, Variant, HGVSg, HGVSc, HGVSp\n",
            "\n",
            "Enter category: solved status\n",
            "Enter item in category 'Solved Status': confirmed\n",
            "Query Results for item (7 records(s) found):\n"
          ]
        }
      ],
      "source": [
        "# Query\n",
        "query_result = query(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***export_to_excel*** function to export the query result into a file kept on the mock local drive."
      ],
      "metadata": {
        "id": "sbfeceYgqjAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "id": "dz5s_DIsKz-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a60839-1d17-48ad-ea83-b0eb23138884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter output file name: unvalidated\n",
            "Data successfully exported to /content/drive/MyDrive/Variant_DB/Mock_Local_Drive/Results/unvalidated.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Export\n",
        "output_name = input(\"Enter output file name: \")\n",
        "output_file = f\"{local_path}Results/{output_name}.xlsx\"\n",
        "export_to_excel(query_result, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variant Validation\n",
        "validation_input_name = input(\"Enter input file name: \")\n",
        "validation_input = f\"{local_path}Results/{validation_input_name}.xlsx\"\n",
        "\n",
        "validation_output_name = input(\"Enter output file name: \")\n",
        "validation_output = f\"{local_path}Results/{validation_output_name}.xlsx\"\n",
        "\n",
        "validate_variants(validation_input, validation_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhr9OA4Wx9EN",
        "outputId": "e6e44f37-3987-421b-d9e3-576f37bf81f7"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter input file name: unvalidated\n",
            "Enter output file name: validated\n",
            "Validating variants...\n",
            "Validation results saved to /content/drive/MyDrive/Variant_DB/Mock_Local_Drive/Results/validated.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dVt3VlqC3Spq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}