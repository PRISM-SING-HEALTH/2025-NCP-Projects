{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRISM-SING-HEALTH/2025-NCP-Projects/blob/main/NCP_Variant_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "FaS8PvOOncrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains Python standard imports required for the program.\n",
        "\n"
      ],
      "metadata": {
        "id": "keKog42wnkb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml"
      ],
      "metadata": {
        "id": "Kn4RxO80npey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains Python imports used for unit testing and are not required for the main program."
      ],
      "metadata": {
        "id": "H6Z3r-NrnraM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-woNJnevcob"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "from unittest.mock import patch, MagicMock\n",
        "import tempfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4UL2tWf3fgb"
      },
      "source": [
        "# functions.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFmHoNpn36Oj"
      },
      "source": [
        "This section contains Python functions along with their documentation, that are used in the main program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoDuKlkc34Wk"
      },
      "outputs": [],
      "source": [
        "def load_config(config_file):\n",
        "    \"\"\"\n",
        "    Loads configuration from a YAML file and validates required keys.\n",
        "\n",
        "    Args:\n",
        "        config_file (str): Path to the YAML configuration file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Configuration settings.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the YAML configuration file is not found.\n",
        "        ValueError: If the YAML file cannot be parsed or required keys are missing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(config_file, 'r') as file:\n",
        "            config = yaml.safe_load(file)\n",
        "\n",
        "        # Validate required keys\n",
        "        required_keys = ['base_path', 'local_path', 'files']\n",
        "        missing_keys = [key for key in required_keys if key not in config]\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required configuration keys: {', '.join(missing_keys)}\")\n",
        "\n",
        "        print(\"Configuration loaded successfully.\")\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The configuration file '{config_file}' was not found.\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error: Failed to parse the YAML configuration file. Details: {e}\")\n",
        "        raise\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KvTuU8M8WP9"
      },
      "outputs": [],
      "source": [
        "def standardise_dataframe(df, columns):\n",
        "  \"\"\"\n",
        "  Converts unstandardised dataframe to standardised dataframe.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "    columns (list): List of standardised column names.\n",
        "\n",
        "  Returns:\n",
        "    standardised_df (pd.DataFrame): Standardized dataframe.\n",
        "  \"\"\"\n",
        "  standardised_df = df.reindex(columns = columns, fill_value = np.nan)\n",
        "  return standardised_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query(df):\n",
        "  \"\"\"\n",
        "  Queries dataframe based on user input. Has case insensitive comparison.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "\n",
        "  Returns:\n",
        "    query_result (pd.DataFrame): Query result.\n",
        "  \"\"\"\n",
        "  print('\\n### Query Dataframe ###')\n",
        "  print(f\"Available categories: {', '.join(df.columns)}\\n\")\n",
        "\n",
        "  try:\n",
        "    # User input category\n",
        "    category = input('Enter category: ').strip()\n",
        "    category_lower = category.lower() # convert to lower case for non-case sens.\n",
        "\n",
        "    # Check if category exists\n",
        "    categories_lower = [col.lower() for col in df.columns]\n",
        "    if category_lower not in categories_lower:\n",
        "      raise ValueError(f\"Invalid category '{category}'. Please choose from available categories.\")\n",
        "\n",
        "    # Find actual column name (case sens match)\n",
        "    category_actual = df.columns[categories_lower.index(category_lower)]\n",
        "\n",
        "    # User input item (case insensitive and substring search)\n",
        "    item = input(f\"Enter item in category '{category_actual}': \").strip()\n",
        "\n",
        "    # Handle empty item input\n",
        "    if not item:\n",
        "      print(\"Error: item cannot be empty.\")\n",
        "      return None\n",
        "\n",
        "    # Query dataframe with case insensitive comparison\n",
        "    query_result = df[df[category_actual].astype(str).str.contains(item, case = False, na = False)]\n",
        "\n",
        "    if query_result.empty:\n",
        "      print(f\"No results found for item containing '{item}' in category '{category_actual}'.\")\n",
        "    else:\n",
        "      print(f\"Query Results for item ({len(query_result)} records(s) found):\")\n",
        "      return query_result\n",
        "\n",
        "  except ValueError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "BDKho2W2cJ43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_excel(df, output_path):\n",
        "    \"\"\"\n",
        "    Exports the given DataFrame to an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to export.\n",
        "        output_path (str): Path where the Excel file will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_filled = df.fillna('NA')\n",
        "        df_filled.to_excel(output_path, index=False)\n",
        "        print(f\"Data successfully exported to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while exporting to Excel: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "cpELaUKfkG_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVt3VlqC3Spq"
      },
      "source": [
        "# test_cases.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA0zVkZe4MNC"
      },
      "source": [
        "This section contains test cases for each function, using Python in-built 'unittest' framework. The test cases ensures the function works as intended for majority of user cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OnayuWx41Gc"
      },
      "outputs": [],
      "source": [
        "class TestLoadConfig(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Set up temporary files and test data.\n",
        "        \"\"\"\n",
        "        self.valid_config = {\n",
        "            'base_path': '/data/',\n",
        "            'local_path': '/local/',\n",
        "            'files': {\n",
        "                'file1': 'file1.xlsx',\n",
        "                'file2': 'file2.xlsx'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.missing_keys_config = {\n",
        "            'base_path': '/data/',\n",
        "            'files': {\n",
        "                'file1': 'file1.xlsx'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create temporary YAML files for testing\n",
        "        self.valid_config_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.valid_config_file.write(yaml.dump(self.valid_config).encode())\n",
        "        self.valid_config_file.close()\n",
        "\n",
        "        self.missing_keys_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.missing_keys_file.write(yaml.dump(self.missing_keys_config).encode())\n",
        "        self.missing_keys_file.close()\n",
        "\n",
        "        self.invalid_yaml_file = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')\n",
        "        self.invalid_yaml_file.write(b\"{invalid_yaml: [missing, closing, brace\")\n",
        "        self.invalid_yaml_file.close()\n",
        "\n",
        "        self.nonexistent_file = '/nonexistent/config.yaml'\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"\n",
        "        Clean up temporary files after tests.\n",
        "        \"\"\"\n",
        "        os.remove(self.valid_config_file.name)\n",
        "        os.remove(self.missing_keys_file.name)\n",
        "        os.remove(self.invalid_yaml_file.name)\n",
        "\n",
        "    def test_valid_config(self):\n",
        "        \"\"\"\n",
        "        Test that a valid configuration file is loaded successfully.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid config file loaded.\")\n",
        "        config = load_config(self.valid_config_file.name)\n",
        "        self.assertEqual(config, self.valid_config)\n",
        "\n",
        "    def test_missing_keys(self):\n",
        "        \"\"\"\n",
        "        Test that a configuration file with missing keys raises a ValueError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Missing keys.\")\n",
        "        with self.assertRaises(ValueError) as context:\n",
        "            load_config(self.missing_keys_file.name)\n",
        "        self.assertIn(\"Missing required configuration keys\", str(context.exception))\n",
        "\n",
        "    def test_invalid_yaml(self):\n",
        "        \"\"\"\n",
        "        Test that an invalid YAML file raises a yaml.YAMLError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Invalid YAML file.\")\n",
        "        with self.assertRaises(yaml.YAMLError):\n",
        "            load_config(self.invalid_yaml_file.name)\n",
        "\n",
        "    def test_file_not_found(self):\n",
        "        \"\"\"\n",
        "        Test that a non-existent file raises a FileNotFoundError.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: File not found.\")\n",
        "        with self.assertRaises(FileNotFoundError):\n",
        "            load_config(self.nonexistent_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0r_1igA8c3l"
      },
      "outputs": [],
      "source": [
        "class TestStandardiseDataFrame(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Define a sample DataFrame and standard columns\n",
        "        self.input_df = pd.DataFrame({\n",
        "            \"A\": [1, 2, 3],\n",
        "            \"B\": [4, 5, 6],\n",
        "            \"C\": [7, 8, 9]\n",
        "        })\n",
        "\n",
        "        self.standard_columns = [\"A\", \"B\", \"C\", \"D\", \"E\"]  # Standardised columns to test against\n",
        "\n",
        "    def test_standardise_dataframe_adds_missing_columns(self):\n",
        "        \"\"\"\n",
        "        Test that missing columns are added with NaN values.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Missing columns added with NaN values.\")\n",
        "        result_df = standardise_dataframe(self.input_df, self.standard_columns)\n",
        "\n",
        "        # Check that the columns in the result match the standard columns\n",
        "        self.assertListEqual(list(result_df.columns), self.standard_columns)\n",
        "\n",
        "        # Check that the new columns contain NaN\n",
        "        self.assertTrue(result_df[\"D\"].isnull().all())\n",
        "        self.assertTrue(result_df[\"E\"].isnull().all())\n",
        "\n",
        "    def test_standardise_dataframe_removes_extra_columns(self):\n",
        "        \"\"\"\n",
        "        Test that extra columns in the input DataFrame are removed.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Extra columns in input dataframe are removed.\")\n",
        "        result_df = standardise_dataframe(self.input_df, [\"A\", \"B\"])  # Subset of standard columns\n",
        "\n",
        "        # Check that only the specified columns remain\n",
        "        self.assertListEqual(list(result_df.columns), [\"A\", \"B\"])\n",
        "\n",
        "    def test_standardise_dataframe_keeps_original_data(self):\n",
        "        \"\"\"\n",
        "        Test that the original data in the columns is preserved.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Original data in columns is preserved.\")\n",
        "        result_df = standardise_dataframe(self.input_df, self.standard_columns)\n",
        "\n",
        "        # Check that data in original columns remains unchanged\n",
        "        pd.testing.assert_series_equal(result_df[\"A\"], self.input_df[\"A\"])\n",
        "        pd.testing.assert_series_equal(result_df[\"B\"], self.input_df[\"B\"])\n",
        "        pd.testing.assert_series_equal(result_df[\"C\"], self.input_df[\"C\"])\n",
        "\n",
        "    def test_standardise_dataframe_empty_dataframe(self):\n",
        "        \"\"\"\n",
        "        Test behavior with an empty input DataFrame.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Empty input dataframe.\")\n",
        "        empty_df = pd.DataFrame()\n",
        "        result_df = standardise_dataframe(empty_df, self.standard_columns)\n",
        "\n",
        "        # Check that all standard columns are present and filled with NaN\n",
        "        self.assertListEqual(list(result_df.columns), self.standard_columns)\n",
        "        self.assertTrue(result_df.isnull().all().all())\n",
        "\n",
        "    def test_standardise_dataframe_no_columns(self):\n",
        "        \"\"\"\n",
        "        Test behavior when no columns are specified.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: No column specified.\")\n",
        "        result_df = standardise_dataframe(self.input_df, [])\n",
        "\n",
        "        # Check that the resulting DataFrame has no columns\n",
        "        self.assertTrue(result_df.empty)\n",
        "        self.assertListEqual(list(result_df.columns), [])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestQueryFunction(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample DataFrame for testing\n",
        "        self.df = pd.DataFrame({\n",
        "            'CategoryA': ['Apple', 'Banana', 'Cherry', 'Apple Pie'],\n",
        "            'CategoryB': ['Dog', 'Cat', 'Horse', 'Fish']\n",
        "        })\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', 'Apple'])\n",
        "    def test_valid_query_case_insensitive(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test valid query with case-insensitive comparison.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid query with case-insensitive comparison\")\n",
        "        result = query(self.df)\n",
        "        expected = self.df[self.df['CategoryA'].str.contains('Apple', case=False, na=False)]\n",
        "        pd.testing.assert_frame_equal(result, expected)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryB', 'cat'])\n",
        "    def test_valid_query_substring_search(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test valid query with substring matching.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Valid query with substring matching\")\n",
        "        result = query(self.df)\n",
        "        expected = self.df[self.df['CategoryB'].str.contains('cat', case=False, na=False)]\n",
        "        pd.testing.assert_frame_equal(result, expected)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['InvalidCategory', 'Apple'])\n",
        "    def test_invalid_category(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying an invalid category.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Invalid category\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', 'NonexistentItem'])\n",
        "    def test_no_results_found(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying an item that doesn't exist in the DataFrame.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: No results found.\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)\n",
        "\n",
        "    @patch('builtins.input', side_effect=['CategoryA', ''])\n",
        "    def test_empty_item(self, mock_input):\n",
        "        \"\"\"\n",
        "        Test querying with an empty item.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Empty item.\")\n",
        "        result = query(self.df)\n",
        "        self.assertIsNone(result)"
      ],
      "metadata": {
        "id": "vIH0-DPgdfXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestExportToExcel(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample DataFrame for testing\n",
        "        self.sample_df = pd.DataFrame({\n",
        "            \"A\": [1, 2, None],\n",
        "            \"B\": [\"Test\", None, \"Data\"]\n",
        "        })\n",
        "        self.output_path = \"test_output.xlsx\"\n",
        "\n",
        "    @patch(\"pandas.DataFrame.to_excel\")\n",
        "    def test_export_to_excel_success(self, mock_to_excel):\n",
        "        \"\"\"\n",
        "        Test successful export of DataFrame to an Excel file.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Export dataframe to excel file.\")\n",
        "        # Call the function\n",
        "        export_to_excel(self.sample_df, self.output_path)\n",
        "\n",
        "        # Assert that `to_excel` was called with correct arguments\n",
        "        mock_to_excel.assert_called_once_with(self.output_path, index=False)\n",
        "\n",
        "    @patch(\"pandas.DataFrame.to_excel\", side_effect=PermissionError(\"Permission denied\"))\n",
        "    def test_export_to_excel_permission_error(self, mock_to_excel):\n",
        "        \"\"\"\n",
        "        Test export when a PermissionError occurs.\n",
        "        \"\"\"\n",
        "        print(\"\\nTest Case: Export with PermissionError.\")\n",
        "        with self.assertRaises(PermissionError):\n",
        "            export_to_excel(self.sample_df, self.output_path)\n",
        "\n",
        "    def tearDown(self):\n",
        "        # Clean up the test output file if it was accidentally created\n",
        "        if os.path.exists(self.output_path):\n",
        "            os.remove(self.output_path)"
      ],
      "metadata": {
        "id": "aZzkF2kRkQ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Run tests without exiting"
      ],
      "metadata": {
        "id": "hAb1njakfaCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCeM1XPC3oPy"
      },
      "source": [
        "# main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0_nVt4o4kRG"
      },
      "source": [
        "This section contains the main program prototype, which will be later transferred over to VSCode once everything is working as intended. The first part loads the configuration from the YAML file and defines the standard columns for the program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JCnmuPNvniD"
      },
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config = load_config('/content/drive/MyDrive/Variant_DB/Mock_Local_Drive/config.yaml')\n",
        "\n",
        "# Initialise file paths variables for excel files\n",
        "base_path = config['base_path']\n",
        "local_path = config['local_path']\n",
        "files = config['files']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_QRs2CDqtdq"
      },
      "outputs": [],
      "source": [
        "# Define standard columns\n",
        "standard_columns = [\n",
        "    'MRN',\n",
        "    'Patient Name',\n",
        "    'Phenotype',\n",
        "    'Solved Status',\n",
        "    'Gene',\n",
        "    'Transcript',\n",
        "    'Variant',\n",
        "    'HGVSg',\n",
        "    'HGVSc',\n",
        "    'HGVSp'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part imports the data into 5 separate, unsorted dataframes."
      ],
      "metadata": {
        "id": "iCMsCWSpo2NI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCnw8yNvEMj_"
      },
      "outputs": [],
      "source": [
        "# Importing data into dataframes\n",
        "print('Importing data...')\n",
        "\n",
        "try:\n",
        "  # Lab Cases\n",
        "  lab_cases_df = pd.read_excel(\n",
        "      f\"{base_path}{files['lab_cases']}\",\n",
        "      sheet_name = 'Sheet1',\n",
        "      header = 0,\n",
        "      usecols = 'F, AF:AI, AL'\n",
        "  )\n",
        "\n",
        "  # ATM Summary\n",
        "  atm_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['atm_summary']}\",\n",
        "      sheet_name='SUMMARY',\n",
        "      header = None,\n",
        "      usecols = 'A:B'\n",
        "  )\n",
        "\n",
        "  # Additional filtering due to unconventional formatting\n",
        "  print('Performing additional filtering...')\n",
        "  atm_summary_df_filtered = atm_summary_df.iloc[9:16]  # Ensure rows are correctly indexed\n",
        "  atm_summary_df_filtered.reset_index(drop=True, inplace=True)  # Reset index for clean output\n",
        "\n",
        "  # Convert to a structured DataFrame\n",
        "  print('Transforming into structured dataframe...')\n",
        "  atm_structured_df = atm_summary_df_filtered.set_index(0).T  # Set column 0 as the header, transpose\n",
        "  atm_structured_df.reset_index(drop=True, inplace=True)  # Reset index for clean final DataFrame\n",
        "\n",
        "  # Invitae Summary\n",
        "  invitae_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['invitae_summary']}\",\n",
        "      sheet_name = 'Invitae list header',\n",
        "      header = 0,\n",
        "      usecols = 'E:F, M:T'\n",
        "  )\n",
        "\n",
        "  # Clinical Summary\n",
        "  clinical_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['clinical_summary']}\",\n",
        "      sheet_name = '11 Dec',\n",
        "      header = 0,\n",
        "      usecols = 'D:E, K')\n",
        "\n",
        "  # Research Summary\n",
        "  research_summary_df = pd.read_excel(\n",
        "      f\"{base_path}{files['research_summary']}\",\n",
        "      sheet_name = 'Overall List',\n",
        "      header = 2,\n",
        "      usecols = 'B, W:X, AF, AS:AT, AV:AW')\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "  print(f\"File not found: {e}\")\n",
        "except ValueError as e:\n",
        "  print(f\"Error reading sheet or invalid data format: {e}\")\n",
        "\n",
        "print('Data import complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "pL1WIYZ4o-BZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TE9tZBvhLKI"
      },
      "outputs": [],
      "source": [
        "# Print dataframes\n",
        "#print('Printing dataframes...')\n",
        "\n",
        "#print('\\nLab Cases')\n",
        "#print(lab_cases_df)\n",
        "\n",
        "#print('\\nATM Summary')\n",
        "#print(atm_structured_df)\n",
        "\n",
        "#print('\\nInvitae Summary')\n",
        "#print(invitae_summary_df)\n",
        "\n",
        "#print('\\nClinical Summary')\n",
        "#print(clinical_summary_df)\n",
        "\n",
        "#print('\\nResearch Summary')\n",
        "#print(research_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses key-value pairs to rename the columns from the imported dataframes to the standardised columns."
      ],
      "metadata": {
        "id": "crVbsanWpGKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLRt0qwiIKBZ"
      },
      "outputs": [],
      "source": [
        "# Standardise columns across dataframes\n",
        "print('Standardising columns across dataframes...')\n",
        "\n",
        "# Lab Cases\n",
        "lab_cases_cols = {\n",
        "    'Number variants detected'    :'Var Count',\n",
        "    'Variant_1_gene'              :'Gene',\n",
        "    'Variant_1_HGVSg'             :'HGVSg',\n",
        "    'Variant_1_HGVSc'             :'HGVSc',\n",
        "    'Variant_1_HGVSp'             :'HGVSp',\n",
        "    'Variant_1_zygosity'          :'Zygosity',\n",
        "    'Variant_1_inheritance'       :'Inheritance',\n",
        "    'Variant_1_Validation_Status' :'Solved Status'\n",
        "}\n",
        "\n",
        "lab_cases_df.rename(columns = lab_cases_cols, inplace = True)\n",
        "\n",
        "# ATM Summary\n",
        "atm_structured_df_cols = {\n",
        "    'HGVS_Genomic_GRCh38/hg38'            :'HSVSg',\n",
        "    'HGVS_MANE Select_Transcript_RefSeq'  :'Transcript',\n",
        "    'HGVS_MANE Select_cDNA'               :'HGVSc',\n",
        "    'HGVS_MANE Select_protein'            :'HGVSp',\n",
        "    'HUGO gene symbol'                    :'Gene',\n",
        "}\n",
        "\n",
        "atm_structured_df.rename(columns = atm_structured_df_cols, inplace = True)\n",
        "\n",
        "# Invitae Summary\n",
        "invitae_summary_df_cols = {\n",
        "    'Patient ID (MRN)'  :'MRN',\n",
        "    'Patient Name'      :'Patient Name',\n",
        "    'Result'            :'Solved Status',\n",
        "    'Gene'              :'Gene',\n",
        "    'Transcript'        :'Transcript',\n",
        "    'Variant'           :'Variant',\n",
        "    'HGVSc'             :'HGVSc',\n",
        "    'Protein Change'    :'HGVSp',\n",
        "}\n",
        "\n",
        "invitae_summary_df.rename(columns = invitae_summary_df_cols, inplace = True)\n",
        "\n",
        "# Clinical Summary\n",
        "clinical_summary_df_cols = {\n",
        "    'Identification No.'        :'MRN',\n",
        "    'Medical Prob description'  :'Phenotype',\n",
        "    'Patient Name'              :'Patient Name'\n",
        "}\n",
        "\n",
        "clinical_summary_df.rename(columns = clinical_summary_df_cols, inplace = True)\n",
        "\n",
        "# Research Summary\n",
        "research_summary_df_cols = {\n",
        "    'IC No (MRN)'         :'MRN',\n",
        "    'Name'                :'Patient Name',\n",
        "    'Candidate gene (1)'  :'Gene',\n",
        "    'Transcript (1)'      :'Transcript',\n",
        "    'cDNA (1)'            :'HGVSc',\n",
        "    'Protein (1)'         :'HGVSp',\n",
        "    'AUTO STATUS'         :'Solved Status'\n",
        "}\n",
        "\n",
        "research_summary_df.rename(columns = research_summary_df_cols, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "iTKUSXvWpR1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x-Sz1WP8kvV"
      },
      "outputs": [],
      "source": [
        "# Print dataframes after renaming\n",
        "#print('Printing dataframes...')\n",
        "\n",
        "#print('\\nLab Cases')\n",
        "#print(lab_cases_df)\n",
        "\n",
        "#print('\\nATM Summary')\n",
        "#print(atm_structured_df)\n",
        "\n",
        "#print('\\nInvitae Summary')\n",
        "#print(invitae_summary_df)\n",
        "\n",
        "#print('\\nClinical Summary')\n",
        "#print(clinical_summary_df)\n",
        "\n",
        "#print('\\nResearch Summary')\n",
        "#print(research_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***standardise_dataframe*** function to sort all of the imported, renamed dataframes into the standardised format for easier integration into a singular dataframe later on."
      ],
      "metadata": {
        "id": "DaxEV5V-pp0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGfg0F1cxdkj"
      },
      "outputs": [],
      "source": [
        "print('Standardising all dataframes...')\n",
        "\n",
        "lab_cases_df_standard = standardise_dataframe(lab_cases_df, standard_columns)\n",
        "atm_structured_df_standard = standardise_dataframe(atm_structured_df, standard_columns)\n",
        "invitae_summary_df_standard = standardise_dataframe(invitae_summary_df, standard_columns)\n",
        "clinical_summary_df_standard = standardise_dataframe(clinical_summary_df, standard_columns)\n",
        "research_summary_df_standard = standardise_dataframe(research_summary_df, standard_columns)\n",
        "\n",
        "print('Standardisation complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part verifies the import with print statements. Comment out as needed."
      ],
      "metadata": {
        "id": "5CCkO49HqJVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrq6O3yrxlns"
      },
      "outputs": [],
      "source": [
        "# Print dataframes to verify\n",
        "#print('Individual dataframes:')\n",
        "#print('\\nLab Cases:')\n",
        "#print(lab_cases_df_standard.head())\n",
        "\n",
        "#print('\\nATM:')\n",
        "#print(atm_structured_df_standard.head())\n",
        "\n",
        "#print('\\nInvitae:')\n",
        "#print(invitae_summary_df_standard.head())\n",
        "\n",
        "#print('\\nClinical:')\n",
        "#print(clinical_summary_df_standard.head())\n",
        "\n",
        "#print('\\nResearch:')\n",
        "#print(research_summary_df_standard.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part combines all of the standardised dataframes into one singular dataframe."
      ],
      "metadata": {
        "id": "WVNB54deqL0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQd3sGenNIzt"
      },
      "outputs": [],
      "source": [
        "# Combining dataframes into a single dataframe\n",
        "print('\\nCombining dataframes...')\n",
        "combined_df = pd.concat([\n",
        "    lab_cases_df_standard,\n",
        "    atm_structured_df_standard,\n",
        "    invitae_summary_df_standard,\n",
        "    clinical_summary_df_standard,\n",
        "    research_summary_df_standard\n",
        "], ignore_index = True)\n",
        "\n",
        "print('All dataframes combined successfully.')\n",
        "print('Combined dataframe:')\n",
        "print(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***query*** function for the user to initiate a query using the new singular dataframe."
      ],
      "metadata": {
        "id": "L8-AGsZDqU90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8dZt3bQKo7i"
      },
      "outputs": [],
      "source": [
        "# Query\n",
        "query_result = query(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the ***export_to_excel*** function to export the query result into a file kept on the mock local drive."
      ],
      "metadata": {
        "id": "sbfeceYgqjAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz5s_DIsKz-Y"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "output_file = f\"{local_path}Results/combined_data.xlsx\" # Fix to be dynamic.\n",
        "export_to_excel(query_result, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "S4UL2tWf3fgb",
        "dVt3VlqC3Spq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}